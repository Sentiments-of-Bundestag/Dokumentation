\section{Implementierung und Bereitstellung}\label{sec:02_04_implementierung_bereitstellung}

\subsection{Implementierung des Crawlers}
Für die Umsetzung des Crawlers wurde die Entscheidung getroffen eine Webanwendung zu implementieren. Für dieses Vorhaben wurde Spring Boot~\cite{SpringBoot242} gewählt, welches ein auf der Programmiersprache Java basierendes Framework für die Entwicklung von Webanwendungen ist. Mithilfe des Frameworks lassen sich Konfigurationen für REST-APIs, Datenbanktreiber, etc. über den Spring-Initializer vereinfachen. Außerdem verfügt Spring Boot über einen eigenen Task-Scheduler der für den Crawl-Manager verwendet werden kann. Die finale Lösung beinhaltet vier Packages: \\\textbf{crawler} mit den Funktionalitäten für den Crawl-Prozess und das Parsen, \textbf{models} und \textbf{repositories} für den DB-Manager. Des Weiteren das Package \textbf{web} mit den Controllern für die Rest-Schnittstelle und den Services für den Crawl-Manager. Das gesamte Projekt-Verzeichnis inklusive des Codes ist auf dem Github-Repository \textit{Sentiments-of-Bundestag/Crawler}~\cite{Crawler2021} zu finden. Für den Download bestimmter Informationen ist es nötig auf Javascript basierende Inhalte auszuführen. Um dies durchzuführen wird die \\\textbf{HtmlUnit}-Bibliothek~\cite{HtmlUnit2021} genutzt. (siehe ~\ref{subsubsec:techAnforderungen}).

\subsection{Bereitstellung der Lösung}
Der Crawler wurde auf dem Virtual-Server der Gruppe 1 (141.45.146.161) an der HTW als Ubuntu-Service bereitgestellt. Dieser ist aus Sicherheitsgründen nur im HTW-Netzwerk sichtbar. Der Crawler-Service liefert eine REST-Schnittstelle, die jedoch aus Sicherheitsgründen nur auf dem infosys1 Rechner zugänglich ist. Die Konfiguration des Crawler-Services, die unter \textit{/etc/systemd/system/crawler.service} angelegt wird, sieht wie folgt aus (Start des Crawlers: systemctl enable crawler):
\begin{lstlisting}
[Unit]
Description=Crawler App
User=local
[Service]
ExecStart=/usr/bin/java -jar ../Crawler-1.0-SNAPSHOT.jar
Restart=always
SyslogIdentifier=crawler
[Install]
WantedBy=multi-user.target
\end{lstlisting}
Da für den Crawler eine MongoDB benötigt wird, soll diese auch bei der Bereitstellung konfiguriert werden. Dabei muss sichergestellt werden, dass der Zugriff auf die Datenbank nur mit den entsprechenden Zugangsdaten erfolgt. Somit ist zu beachten, dass nach dem Hinzufügen eines neuen Admin-Users die Security-Konfiguration von mongod unter \textit{sudo nano /etc/mongod.conf} auf \textit{authorization: enabled} umgestellt werden. Da die MongoDB von der Gruppe 2 (Kommunikationsmodell) verwendet wird, sollen dafür entsprechend die Firewall-Regeln angepasst werden. Der nachfolgende Auszug aus der Konfiguration-Datei (/root/Firewall.sh) zeigt wie dies beispielhaft erfolgen sollte.
\begin{lstlisting}
# Erlaube zugang im 145.45.x.x port 27017 MongoDB
iptables -A INPUT -s 141.45.0.0/16 -p tcp -m tcp --dport 27017 -j ACCEPT
\end{lstlisting}
Der bereitgestellte Crawler basiert auf Java 11 und ist somit eine plattformunabhängige Lösung, die auf allen Betriebssystemen angeboten werden kann. Die Steuerung des Crawlers erfolgt ausschließlich über die REST-Schnittstelle mit folgenden Abfragen (hier mit curl in der Shell-Konsole):
\begin{lstlisting}
# opendata: https://www.bundestag.de/services/opendata
# Start a default crawl to opendata
$ curl -X POST http://localhost:8080/task/default
# Start the default cron process to opendata: Mon - Fri, 23
$ curl -X POST http://localhost:8080/task/cron
# List all running tasks
$ curl -X GET http://localhost:8080/tasks 
# Cancel planed task
$ curl -X POST http://localhost:8080/task/cancel/{task_id}
# Cancel all planed tasks
$ curl -X POST http://localhost:8080/tasks/cancel
\end{lstlisting}
Der Crawler auf dem infosys1-Server (infosys1.f4.htw-berlin.de) läuft seit dem 27.11.2020, bis auf kleine Unterbrechungen aufgrund von Aktualisierungen, problemlos und konnte zum Zeitpunkt der Verfassung dieser Dokumentation schon über 200 Protokolle herunterladen, parsen und in der MongoDB speichern. Des Weiteren wurden Stammdaten von 4.086 Abgeordneten gesammelt und mehr als 500 URLs durchsucht. Der Datenaustausch mit der Gruppe des Kommunikationsmodells war erfolgreich, da diese ohne Probleme auf die bereitgestellte MongoDB zugreifen konnten und die Benachrichtigungen ebenfalls fehlerfrei empfangen wurden.\\
Aus den vom Crawler gesammelten Daten wird nun durch Gruppe 2 ein Kommunikationsmodell entwickelt, welches von anderen Gruppen für weitere Verarbeitungen und Analysen verwendet werden kann. Der Aufbau dieses Kommunikationsmodells wird nun von Gruppe 2 im nächsten Kapitel aufgegriffen. 